{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bridal-dictionary",
   "metadata": {},
   "source": [
    "xgb 参数解释：https://blog.csdn.net/iyuanshuo/article/details/80142730"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-rogers",
   "metadata": {},
   "source": [
    "调参过程：https://blog.csdn.net/han_xiaoyang/article/details/52663170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "parliamentary-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-29\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import entropy\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import datetime\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "nowtime = datetime.date.today()\n",
    "nowtime = str(nowtime)[-5:]\n",
    "print(nowtime)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/'\n",
    "\n",
    "def load_dataset(DATA_PATH):\n",
    "    #train_label = pd.read_csv(DATA_PATH + 'train.csv')['isDefault']\n",
    "    train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "    test = pd.read_csv(DATA_PATH + 'testA.csv')\n",
    "    train = train.sample(2000)\n",
    "    test = test.sample(500)\n",
    "    train.to_csv(DATA_PATH + 'target.csv')\n",
    "    train_label = pd.read_csv(DATA_PATH + 'target.csv')['isDefault']\n",
    "    \n",
    "    feats = [f for f in train.columns if f not in ['isDefault']]\n",
    "    # train = train[feats]\n",
    "    test = test[feats]\n",
    "    print('train.shape', train.shape)\n",
    "    print('test.shape', test.shape)\n",
    "\n",
    "    return train_label, train, test\n",
    "\n",
    "\n",
    "# 处理时间\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400 * day + 3600 * hour + 60 * minute + second\n",
    "\n",
    "\n",
    "def transform_day(date1):\n",
    "    date2 = \"2020-01-01\"\n",
    "    date1 = time.strptime(date1, \"%Y-%m-%d\")\n",
    "    date2 = time.strptime(date2, \"%Y-%m-%d\")\n",
    "\n",
    "    # 根据上面需要计算日期还是日期时间，来确定需要几个数组段。下标0表示年，小标1表示月，依次类推...\n",
    "    # date1=datetime.datetime(date1[0],date1[1],date1[2],date1[3],date1[4],date1[5])\n",
    "    # date2=datetime.datetime(date2[0],date2[1],date2[2],date2[3],date2[4],date2[5])\n",
    "    date1 = datetime.datetime(date1[0], date1[1], date1[2])\n",
    "    date2 = datetime.datetime(date2[0], date2[1], date2[2])\n",
    "    # 返回两个变量相差的值，就是相差天数\n",
    "    # print((date2 - date1).days)  # 将天数转成int型\n",
    "    return (date2 - date1).days\n",
    "\n",
    "\n",
    "# transform_day('2007-09-01')\n",
    "\n",
    "def labelEncoder_df(df, features):\n",
    "    for i in features:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        df[i] = encoder.fit_transform(df[i])\n",
    "\n",
    "\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg([('mean','mean'), ('beta','size')])\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target,\n",
    "                        self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.index.get_loc[large_ind], y.index.get_loc[large_ind], X_new.index.get_loc[small_ind], variable, None,\n",
    "                        self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "\n",
    "def employmentLength_deal(x):\n",
    "    if x == r'\\N':\n",
    "        result = -999\n",
    "    elif x == -999:\n",
    "        result = -999\n",
    "    elif x == '-999':\n",
    "        result = -999\n",
    "    elif x == '< 1 year':\n",
    "        result = 0.5\n",
    "    elif x == '10+ years':\n",
    "        result = 12\n",
    "    else:\n",
    "        result = int(x.split(' ')[0][0])\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def earliesCreditLine_month_deal(x):\n",
    "    x = x.split('-')[0]\n",
    "    # print(x)\n",
    "    dict = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10,\n",
    "            'Nov': 11, 'Dec': 12}\n",
    "    result = dict[x]\n",
    "    return result\n",
    "\n",
    "\n",
    "def gradeTrans(x):\n",
    "    dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    result = dict[x]\n",
    "    return result\n",
    "\n",
    "\n",
    "def subGradeTrans(x):\n",
    "    dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    result = dict[x[0]]\n",
    "    result = result * 5 + int(x[1])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def myEntro(x):\n",
    "    \"\"\"\n",
    "        calculate shanno ent of x\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        p = float(x[x == x_value].shape[0]) / x.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "    #     print(x_value,p,logp)\n",
    "    # print(ent)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def myRms(records):\n",
    "    records = list(records)\n",
    "    \"\"\"\n",
    "    均方根值 反映的是有效值而不是平均值\n",
    "    \"\"\"\n",
    "    return np.math.sqrt(sum([x ** 2 for x in records]) / len(records))\n",
    "\n",
    "\n",
    "def myMode(x):\n",
    "    return np.mean(pd.Series.mode(x))\n",
    "\n",
    "\n",
    "def myQ25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "\n",
    "def myQ75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "\n",
    "def myRange(x):\n",
    "    return pd.Series.max(x) - pd.Series.min(x)\n",
    "\n",
    "\n",
    "# 预处理\n",
    "def data_preprocess(DATA_PATH):\n",
    "    train_label, train, test = load_dataset(DATA_PATH=DATA_PATH)\n",
    "    # 拼接数据\n",
    "\n",
    "    data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    print('初始拼接后：', data.shape)\n",
    "    # n_feat = [f for f in data.columns if f[0] == 'n']\n",
    "\n",
    "    n_feat = ['n0', 'n1', 'n2', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', ]\n",
    "    # nameList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', 'mode', 'range']\n",
    "    # statList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', myMode, myRange]\n",
    "    nameList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "    statList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "\n",
    "\n",
    "    for i in range(len(nameList)):\n",
    "        data['n_feat_{}'.format(nameList[i])] = data[n_feat].agg(statList[i], axis=1)\n",
    "    print('n特征处理后：', data.shape)\n",
    "\n",
    "\n",
    "    # count编码\n",
    "    count_list = ['subGrade', 'grade', 'postCode', 'regionCode','homeOwnership','title','employmentTitle','employmentLength']\n",
    "    data = count_coding(data, count_list)\n",
    "    print('count编码后：', data.shape)\n",
    "    ### 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "    cross_cat = ['subGrade', 'grade', 'employmentLength', 'term', 'homeOwnership', 'postCode', 'regionCode','employmentTitle','title']\n",
    "    cross_num = ['dti', 'revolBal','revolUtil', 'ficoRangeHigh', 'interestRate', 'loanAmnt', 'installment', 'annualIncome', 'n14',\n",
    "                 'n2', 'n6', 'n9', 'n5', 'n8']\n",
    "\n",
    "    data[['employmentLength']].fillna(-999, inplace=True)\n",
    "\n",
    "#     data = cross_cat_num(data, cross_num, cross_cat)  # 一阶交叉\n",
    "#     print('一阶特征处理后：', data.shape)\n",
    "#     data = cross_qua_cat_num(data)  # 二阶交叉\n",
    "#     print('二阶特征处理后：', data.shape)\n",
    "    # 缺失值处理\n",
    "    for temp in count_list:\n",
    "        del data[temp+'_count']\n",
    "    # num_fill_col = ['employmentLength', 'postCode', ]\n",
    "    cols = ['employmentTitle', 'employmentLength', 'postCode', 'dti', 'pubRecBankruptcies', 'revolUtil', 'title',\n",
    "            'n0', 'n1', 'n2', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14']\n",
    "    for col in cols:\n",
    "        data[col].fillna(r'\\N', inplace=True)\n",
    "    cols = [f for f in cols if f not in ['employmentLength']]\n",
    "    for col in cols:\n",
    "        data[col].replace({r'\\N': -999}, inplace=True)\n",
    "        data[col] = data[col]\n",
    "    # print('缺失值情况：', data.isnull().sum())\n",
    "\n",
    "    data['grade'] = data['grade'].apply(lambda x: gradeTrans(x))\n",
    "    data['subGrade'] = data['subGrade'].apply(lambda x: subGradeTrans(x))\n",
    "    print('1data.shape', data.shape)\n",
    "\n",
    "    data['employmentLength'] = data['employmentLength'].apply(lambda x: employmentLength_deal(x))\n",
    "    data['issueDate_year'] = data['issueDate'].apply(lambda x: int(x.split('-')[0]))\n",
    "    data['issueDate_month'] = data['issueDate'].apply(lambda x: int(x.split('-')[1]))\n",
    "    data['issueDate_day'] = data['issueDate'].apply(lambda x: transform_day(x))\n",
    "    data['issueDate_week'] = data['issueDate_day'].apply(lambda x: int(x % 7) + 1)\n",
    "\n",
    "    print('2_data.shape', data.shape)\n",
    "    data['earliesCreditLine_year'] = data['earliesCreditLine'].apply(lambda x: 2020 - (int(x.split('-')[-1])))\n",
    "    data['earliesCreditLine_month'] = data['earliesCreditLine'].apply(lambda x: earliesCreditLine_month_deal(x))\n",
    "    data['earliesCreditLine_Allmonth'] = data['earliesCreditLine_year'] * 12 - data['earliesCreditLine_month']\n",
    "    del data['issueDate'], data['earliesCreditLine']\n",
    "\n",
    "    print('预处理完毕', data.shape)\n",
    "\n",
    "    return data, train_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kfold_stats_feature(train, test, feats, k):\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=6666)  # 这里最好和后面模型的K折交叉验证保持一致\n",
    "\n",
    "    train['fold'] = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['isDefault'])):\n",
    "        train.loc[val_idx, 'fold'] = fold_\n",
    "\n",
    "    kfold_features = []\n",
    "    for feat in feats:\n",
    "        nums_columns = ['isDefault']\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            kfold_features.append(colname)\n",
    "            train[colname] = None\n",
    "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['isDefault'])):\n",
    "                tmp_trn = train.iloc[trn_idx]\n",
    "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
    "                tmp = train.loc[train.fold == fold_, [feat]]\n",
    "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n",
    "                # fillna\n",
    "                global_mean = train[f].mean()\n",
    "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
    "            train[colname] = train[colname].astype(float)\n",
    "\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            test[colname] = None\n",
    "            order_label = train.groupby([feat])[f].mean()\n",
    "            test[colname] = test[feat].map(order_label)\n",
    "            # fillna\n",
    "            global_mean = train[f].mean()\n",
    "            test[colname] = test[colname].fillna(global_mean)\n",
    "            test[colname] = test[colname].astype(float)\n",
    "    del train['fold']\n",
    "    return train, test\n",
    "\n",
    "def GridSearch(clf, params, X, y):\n",
    "    cscv = GridSearchCV(clf, params, scoring='roc_auc', n_jobs=4, cv=10)\n",
    "    cscv.fit(X, y)\n",
    "    print(cscv.cv_results_)\n",
    "    print(cscv.best_params_)\n",
    "    print(cscv.best_score_)\n",
    "\n",
    "### count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return (df)\n",
    "\n",
    "\n",
    "# 定义交叉特征统计\n",
    "def cross_cat_num(df, num_col, cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return (df)\n",
    "\n",
    "\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['subGrade', 'regionCode'], ['grade', 'regionCode'], ['subGrade', 'postCode'], ['grade', 'postCode'], ['employmentTitle','title'],\n",
    "        ['regionCode','title'], ['postCode','title'], ['homeOwnership','title'], ['homeOwnership','employmentTitle'],['homeOwnership','employmentLength'],\n",
    "        ['regionCode', 'postCode']\n",
    "    ]):\n",
    "        ### 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['id'].transform('count')\n",
    "        ### n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        ### 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return (df)\n",
    "\n",
    "\n",
    "### count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return (df)\n",
    "\n",
    "def gen_basicFea(data):\n",
    "    data['avg_income'] = data['annualIncome'] / data['employmentLength']\n",
    "    data['total_income'] = data['annualIncome'] * data['employmentLength']\n",
    "    data['avg_loanAmnt'] = data['loanAmnt'] / data['term']\n",
    "    data['mean_interestRate'] = data['interestRate'] / data['term']\n",
    "    data['all_installment'] = data['installment'] * data['term']\n",
    "\n",
    "    data['rest_money_rate'] = data['avg_loanAmnt'] / (data['annualIncome'] + 0.1)  # 287个收入为0\n",
    "    data['rest_money'] = data['annualIncome'] - data['avg_loanAmnt']\n",
    "\n",
    "    data['closeAcc'] = data['totalAcc'] - data['openAcc']\n",
    "    data['ficoRange_mean'] = (data['ficoRangeHigh'] + data['ficoRangeLow']) / 2\n",
    "    del data['ficoRangeHigh'], data['ficoRangeLow']\n",
    "\n",
    "    data['rest_pubRec'] = data['pubRec'] - data['pubRecBankruptcies']\n",
    "\n",
    "    data['rest_Revol'] = data['loanAmnt'] - data['revolBal']\n",
    "\n",
    "    data['dis_time'] = data['issueDate_year'] - (2020 - data['earliesCreditLine_year'])\n",
    "    for col in ['employmentTitle', 'grade', 'subGrade', 'regionCode', 'issueDate_month', 'postCode']:\n",
    "        data['{}_count'.format(col)] = data.groupby([col])['id'].transform('count')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def plotroc(train_y, train_pred, test_y, val_pred):\n",
    "    lw = 2\n",
    "    ##train\n",
    "    fpr, tpr, thresholds = roc_curve(train_y.values, train_pred, pos_label=1.0)\n",
    "    train_auc_value = roc_auc_score\n",
    "    roc_auc_score(train_y.values, train_pred)\n",
    "    ##valid\n",
    "    fpr, tpr, thresholds = roc_curve(test_y.values, val_pred, pos_label=1.0)\n",
    "    valid_auc_value= roc_auc_score(test_y.values, val_pred)\n",
    "\n",
    "    return train_auc_value, valid_auc_value\n",
    "\n",
    "\n",
    "def xgb_model(train, target, test, k):\n",
    "    # saveFeature_df = pd.read_csv('../feature/xgb_920_556_5_score.csv',header=None)\n",
    "#     saveFeature_df2 = pd.read_csv('../feature/xgb_09-20_74_0.8_0.6_5_score.csv',header=None)\n",
    "    \n",
    "    # saveFeature_df.columns=['feature','score']\n",
    "#     saveFeature_df2.columns=['feature','score']\n",
    "\n",
    "#     saveFeature_list = list(saveFeature_df['feature'].values)\n",
    "#     saveFeature_list2 = list(saveFeature_df2['feature'].values)\n",
    "    \n",
    "    # saveFeature_list = list(saveFeature_df[saveFeature_df['score']>10]['feature'])\n",
    "    saveFeature_list=list(train.columns)\n",
    "    feats = [f for f in saveFeature_list if f not in ['id', 'isDefault']]\n",
    "    feaNum = len(feats)\n",
    "    print('Current num of features:', len(feats))\n",
    "\n",
    "    seeds = [2020,666666,188888]\n",
    "    output_preds = 0\n",
    "    xgb_oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "        offline_score = []\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        params = {'booster': 'gbtree',\n",
    "                  'objective': 'binary:logistic',\n",
    "                  'eval_metric': 'auc',\n",
    "                  'min_child_weight': 5,\n",
    "                  'max_depth': 8,\n",
    "                  'subsample': ss,\n",
    "                  'colsample_bytree': fs,\n",
    "                  'eta': 0.01,\n",
    "\n",
    "                  'seed': seed,\n",
    "                  'nthread': -1,\n",
    "\n",
    "                  'tree_method': 'hist'\n",
    "                  }\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            \n",
    "            train_y, test_y = target[train_index], target[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            train_matrix = xgb.DMatrix(train_X, label=train_y, missing=np.nan)\n",
    "            valid_matrix = xgb.DMatrix(test_X, label=test_y, missing=np.nan)\n",
    "            test_matrix = xgb.DMatrix(test[feats], missing=np.nan)\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = xgb.train(params, train_matrix, num_boost_round=1000, evals=watchlist, verbose_eval=100,\n",
    "                              early_stopping_rounds=600)\n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            train_pred = model.predict(train_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            xgb_oof_probs[test_index] += val_pred / len(seeds)\n",
    "            # oof_probs[test_index] += val_pred\n",
    "            test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "            # 绘制roc曲线\n",
    "            train_auc_value, valid_auc_value = plotroc(train_y, train_pred, test_y, val_pred)\n",
    "            print('train_auc:{},valid_auc{}'.format(train_auc_value, valid_auc_value))\n",
    "            offline_score.append(valid_auc_value)\n",
    "            print(offline_score)\n",
    "            output_preds += test_pred / k / len(seeds)\n",
    "            \n",
    "#             sub_df = test[['id']].copy()\n",
    "#             sub_df['isDefault'] = output_preds\n",
    "#             off = test[['id']].copy()\n",
    "#             subVal_df = train[['id']].copy()\n",
    "#             subVal_df.loc[test_index,'isDefault'] = xgb_oof_probs[test_index]\n",
    "#             outpath = '../user_data/fold/'\n",
    "#             fold_score = round(valid_auc_value, 5)\n",
    "#             sub_df.to_csv( outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgb.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "#             subVal_df.to_csv(outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgbVal.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"Feature\"] = model.get_fscore().keys()\n",
    "            fold_importance_df[\"importance\"] = model.get_fscore().values()\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "#             print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "#             feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "#             feature_sorted.to_csv('../feature/xgb_{}_{}_{}_{}_score.csv'.format(i+1,nowtime, feaNum, kflod_num))\n",
    "\n",
    "#             if i==3:\n",
    "#                 break\n",
    "#             gc.collect()\n",
    "\n",
    "\n",
    "        print('all_auc:', roc_auc_score(target.values, oof_probs))\n",
    "        print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "        feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "        feature_sorted.to_csv('/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/xgb_importance.csv')\n",
    "        top_features = feature_sorted.index\n",
    "        print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    return output_preds, xgb_oof_probs, np.mean(offline_score), feaNum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "becoming-crown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "green-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "train.shape (2000, 47)\n",
      "test.shape (500, 46)\n",
      "初始拼接后： (2500, 47)\n",
      "n特征处理后： (2500, 53)\n",
      "count编码后： (2500, 61)\n",
      "1data.shape (2500, 53)\n",
      "2_data.shape (2500, 57)\n",
      "预处理完毕 (2500, 58)\n",
      "开始特征工程...\n",
      "data.shape (2500, 74)\n",
      "开始模型训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num0:mean_encode train.shape (2000, 84) (2000, 84)\n",
      "num1:target_encode train.shape (2000, 89) (2000, 89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num2:target_encode train.shape (2000, 109) (2000, 109)\n",
      "输入数据维度： (2000, 104) (2000, 104)\n",
      "Current num of features: 102\n",
      "[0]\ttrain-auc:0.74820\teval-auc:0.65560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-auc:0.93490\teval-auc:0.73970\n",
      "[200]\ttrain-auc:0.96640\teval-auc:0.74446\n",
      "[300]\ttrain-auc:0.98382\teval-auc:0.74593\n",
      "[400]\ttrain-auc:0.99262\teval-auc:0.74562\n",
      "[500]\ttrain-auc:0.99701\teval-auc:0.74221\n",
      "[600]\ttrain-auc:0.99879\teval-auc:0.73459\n",
      "[700]\ttrain-auc:0.99958\teval-auc:0.73285\n",
      "[769]\ttrain-auc:0.99983\teval-auc:0.73029\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7480939664847711\n",
      "[0.7480939664847711]\n",
      "[0]\ttrain-auc:0.76366\teval-auc:0.58812\n",
      "[100]\ttrain-auc:0.93377\teval-auc:0.66078\n",
      "[200]\ttrain-auc:0.96405\teval-auc:0.65626\n",
      "[300]\ttrain-auc:0.98175\teval-auc:0.65649\n",
      "[400]\ttrain-auc:0.99160\teval-auc:0.65641\n",
      "[500]\ttrain-auc:0.99653\teval-auc:0.65614\n",
      "[600]\ttrain-auc:0.99865\teval-auc:0.65184\n",
      "[603]\ttrain-auc:0.99869\teval-auc:0.65200\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.6659119934982003\n",
      "[0.7480939664847711, 0.6659119934982003]\n",
      "[0]\ttrain-auc:0.74976\teval-auc:0.68774\n",
      "[100]\ttrain-auc:0.93825\teval-auc:0.71907\n",
      "[200]\ttrain-auc:0.97007\teval-auc:0.72197\n",
      "[300]\ttrain-auc:0.98665\teval-auc:0.72162\n",
      "[400]\ttrain-auc:0.99443\teval-auc:0.72305\n",
      "[500]\ttrain-auc:0.99786\teval-auc:0.71888\n",
      "[600]\ttrain-auc:0.99911\teval-auc:0.71675\n",
      "[700]\ttrain-auc:0.99971\teval-auc:0.71620\n",
      "[736]\ttrain-auc:0.99979\teval-auc:0.71617\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7253376678664034\n",
      "[0.7480939664847711, 0.6659119934982003, 0.7253376678664034]\n",
      "[0]\ttrain-auc:0.76571\teval-auc:0.58164\n",
      "[100]\ttrain-auc:0.94333\teval-auc:0.66907\n",
      "[200]\ttrain-auc:0.97035\teval-auc:0.67019\n",
      "[300]\ttrain-auc:0.98506\teval-auc:0.66485\n",
      "[400]\ttrain-auc:0.99369\teval-auc:0.65657\n",
      "[500]\ttrain-auc:0.99711\teval-auc:0.65347\n",
      "[600]\ttrain-auc:0.99883\teval-auc:0.64763\n",
      "[700]\ttrain-auc:0.99959\teval-auc:0.64558\n",
      "[748]\ttrain-auc:0.99978\teval-auc:0.64465\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.6737876852819381\n",
      "[0.7480939664847711, 0.6659119934982003, 0.7253376678664034, 0.6737876852819381]\n",
      "[0]\ttrain-auc:0.76412\teval-auc:0.53686\n",
      "[100]\ttrain-auc:0.93164\teval-auc:0.65869\n",
      "[200]\ttrain-auc:0.96529\teval-auc:0.67000\n",
      "[300]\ttrain-auc:0.98429\teval-auc:0.67255\n",
      "[400]\ttrain-auc:0.99394\teval-auc:0.66996\n",
      "[500]\ttrain-auc:0.99751\teval-auc:0.66779\n",
      "[600]\ttrain-auc:0.99898\teval-auc:0.66717\n",
      "[700]\ttrain-auc:0.99962\teval-auc:0.66790\n",
      "[800]\ttrain-auc:0.99989\teval-auc:0.66636\n",
      "[855]\ttrain-auc:0.99994\teval-auc:0.66651\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.6744069042919618\n",
      "[0.7480939664847711, 0.6659119934982003, 0.7253376678664034, 0.6737876852819381, 0.6744069042919618]\n",
      "all_auc: 0.5\n",
      "OOF-MEAN-AUC:0.697508, OOF-STD-AUC:0.032949\n",
      "Feature\n",
      "n_feat_skew                           450.6\n",
      "interestRate                          449.8\n",
      "rest_money_rate                       421.4\n",
      "avg_income                            420.6\n",
      "dti                                   404.4\n",
      "rest_Revol                            384.8\n",
      "regionCode_target_std                 382.6\n",
      "n_feat_std                            362.6\n",
      "total_income                          355.8\n",
      "regionCode_isDefault_kfold_mean       347.6\n",
      "earliesCreditLine_Allmonth            334.0\n",
      "regionCode_pred_0                     332.0\n",
      "installment                           331.4\n",
      "all_installment                       326.6\n",
      "revolUtil                             323.2\n",
      "rest_money                            322.8\n",
      "issueDate_day                         315.8\n",
      "regionCode_target_skew                311.6\n",
      "revolBal                              303.8\n",
      "annualIncome                          303.0\n",
      "n_feat_mean                           298.0\n",
      "ficoRange_mean                        284.8\n",
      "mean_interestRate                     269.6\n",
      "title_target_skew                     266.6\n",
      "earliesCreditLine_month               256.6\n",
      "postCode_pred_0                       254.0\n",
      "postCode_target_std                   239.8\n",
      "postCode_count                        238.6\n",
      "closeAcc                              224.8\n",
      "regionCode_count                      221.2\n",
      "avg_loanAmnt                          220.2\n",
      "title_isDefault_kfold_mean            217.0\n",
      "loanAmnt                              206.4\n",
      "homeOwnership_pred_0                  206.0\n",
      "issueDate_month                       205.4\n",
      "title_target_std                      204.8\n",
      "postCode_isDefault_kfold_mean         201.0\n",
      "n_feat_sum                            201.0\n",
      "regionCode_pred_1                     196.2\n",
      "totalAcc                              193.4\n",
      "postCode_target_skew                  192.8\n",
      "subGrade                              187.0\n",
      "homeOwnership_isDefault_kfold_mean    185.4\n",
      "n6                                    182.4\n",
      "dis_time                              180.0\n",
      "issueDate_month_count                 164.8\n",
      "n_feat_max                            161.6\n",
      "earliesCreditLine_year                161.6\n",
      "title_pred_0                          156.8\n",
      "homeOwnership_target_skew             153.4\n",
      "Name: importance, dtype: float64\n",
      "[0]\ttrain-auc:0.72292\teval-auc:0.58296\n",
      "[100]\ttrain-auc:0.93893\teval-auc:0.72031\n",
      "[200]\ttrain-auc:0.97000\teval-auc:0.71334\n",
      "[300]\ttrain-auc:0.98543\teval-auc:0.71044\n",
      "[400]\ttrain-auc:0.99355\teval-auc:0.70746\n",
      "[500]\ttrain-auc:0.99705\teval-auc:0.70591\n",
      "[600]\ttrain-auc:0.99876\teval-auc:0.69887\n",
      "[626]\ttrain-auc:0.99898\teval-auc:0.69976\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7252602654901505\n",
      "[0.7252602654901505]\n",
      "[0]\ttrain-auc:0.75452\teval-auc:0.59027\n",
      "[100]\ttrain-auc:0.93599\teval-auc:0.68586\n",
      "[200]\ttrain-auc:0.96591\teval-auc:0.68760\n",
      "[300]\ttrain-auc:0.98333\teval-auc:0.68551\n",
      "[400]\ttrain-auc:0.99284\teval-auc:0.68842\n",
      "[500]\ttrain-auc:0.99689\teval-auc:0.68428\n",
      "[600]\ttrain-auc:0.99882\teval-auc:0.68265\n",
      "[700]\ttrain-auc:0.99960\teval-auc:0.67893\n",
      "[755]\ttrain-auc:0.99982\teval-auc:0.67913\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.6918998413251287\n",
      "[0.7252602654901505, 0.6918998413251287]\n",
      "[0]\ttrain-auc:0.72501\teval-auc:0.57601\n",
      "[100]\ttrain-auc:0.93488\teval-auc:0.67031\n",
      "[200]\ttrain-auc:0.96376\teval-auc:0.67417\n",
      "[300]\ttrain-auc:0.98195\teval-auc:0.68006\n",
      "[400]\ttrain-auc:0.99116\teval-auc:0.67766\n",
      "[500]\ttrain-auc:0.99644\teval-auc:0.67909\n",
      "[600]\ttrain-auc:0.99875\teval-auc:0.67735\n",
      "[700]\ttrain-auc:0.99963\teval-auc:0.67561\n",
      "[800]\ttrain-auc:0.99991\teval-auc:0.67441\n",
      "[900]\ttrain-auc:0.99999\teval-auc:0.67607\n",
      "[936]\ttrain-auc:1.00000\teval-auc:0.67584\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.6824180502341423\n",
      "[0.7252602654901505, 0.6918998413251287, 0.6824180502341423]\n",
      "[0]\ttrain-auc:0.73368\teval-auc:0.63468\n",
      "[100]\ttrain-auc:0.93219\teval-auc:0.69356\n",
      "[200]\ttrain-auc:0.96449\teval-auc:0.69387\n",
      "[300]\ttrain-auc:0.98300\teval-auc:0.69999\n",
      "[400]\ttrain-auc:0.99270\teval-auc:0.70247\n",
      "[500]\ttrain-auc:0.99692\teval-auc:0.70452\n",
      "[600]\ttrain-auc:0.99874\teval-auc:0.70394\n",
      "[700]\ttrain-auc:0.99950\teval-auc:0.70386\n",
      "[800]\ttrain-auc:0.99984\teval-auc:0.69879\n",
      "[900]\ttrain-auc:0.99996\teval-auc:0.69724\n",
      "[999]\ttrain-auc:0.99999\teval-auc:0.69507\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7052904524168893\n",
      "[0.7252602654901505, 0.6918998413251287, 0.6824180502341423, 0.7052904524168893]\n",
      "[0]\ttrain-auc:0.70008\teval-auc:0.57907\n",
      "[100]\ttrain-auc:0.93360\teval-auc:0.69066\n",
      "[200]\ttrain-auc:0.96563\teval-auc:0.69283\n",
      "[300]\ttrain-auc:0.98422\teval-auc:0.69407\n",
      "[400]\ttrain-auc:0.99349\teval-auc:0.69925\n",
      "[500]\ttrain-auc:0.99764\teval-auc:0.70239\n",
      "[600]\ttrain-auc:0.99921\teval-auc:0.70107\n",
      "[700]\ttrain-auc:0.99978\teval-auc:0.70614\n",
      "[800]\ttrain-auc:0.99994\teval-auc:0.70595\n",
      "[900]\ttrain-auc:0.99999\teval-auc:0.70328\n",
      "[999]\ttrain-auc:1.00000\teval-auc:0.70347\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7072255118232129\n",
      "[0.7252602654901505, 0.6918998413251287, 0.6824180502341423, 0.7052904524168893, 0.7072255118232129]\n",
      "all_auc: 0.5\n",
      "OOF-MEAN-AUC:0.702419, OOF-STD-AUC:0.014586\n",
      "Feature\n",
      "n_feat_skew                        500.6\n",
      "rest_money_rate                    477.2\n",
      "interestRate                       469.8\n",
      "dti                                452.4\n",
      "avg_income                         443.4\n",
      "rest_Revol                         438.0\n",
      "total_income                       414.6\n",
      "n_feat_std                         413.8\n",
      "all_installment                    413.2\n",
      "rest_money                         406.0\n",
      "regionCode_target_std              405.8\n",
      "revolUtil                          392.2\n",
      "regionCode_pred_0                  378.2\n",
      "regionCode_target_skew             369.8\n",
      "earliesCreditLine_Allmonth         368.2\n",
      "revolBal                           364.4\n",
      "regionCode_isDefault_kfold_mean    350.8\n",
      "installment                        343.6\n",
      "issueDate_day                      333.0\n",
      "mean_interestRate                  329.4\n",
      "annualIncome                       323.8\n",
      "n_feat_mean                        308.0\n",
      "ficoRange_mean                     306.8\n",
      "postCode_target_std                302.6\n",
      "title_target_skew                  296.2\n",
      "earliesCreditLine_month            295.8\n",
      "closeAcc                           280.2\n",
      "postCode_count                     269.2\n",
      "postCode_pred_0                    265.8\n",
      "regionCode_count                   265.6\n",
      "homeOwnership_pred_0               250.0\n",
      "avg_loanAmnt                       245.8\n",
      "title_isDefault_kfold_mean         242.4\n",
      "title_target_std                   235.6\n",
      "postCode_isDefault_kfold_mean      227.0\n",
      "n_feat_sum                         218.8\n",
      "loanAmnt                           217.8\n",
      "regionCode_pred_1                  216.2\n",
      "issueDate_month                    208.8\n",
      "totalAcc                           207.4\n",
      "postCode_target_skew               204.0\n",
      "dis_time                           199.6\n",
      "subGrade                           198.2\n",
      "homeOwnership_target_skew          194.2\n",
      "n6                                 190.2\n",
      "title_pred_0                       190.2\n",
      "postCode_pred_1                    187.4\n",
      "issueDate_week                     185.8\n",
      "earliesCreditLine_year             183.0\n",
      "issueDate_month_count              176.2\n",
      "Name: importance, dtype: float64\n",
      "[0]\ttrain-auc:0.73642\teval-auc:0.60656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-auc:0.93741\teval-auc:0.70324\n",
      "[200]\ttrain-auc:0.96782\teval-auc:0.70831\n",
      "[300]\ttrain-auc:0.98434\teval-auc:0.70390\n",
      "[400]\ttrain-auc:0.99253\teval-auc:0.70138\n",
      "[500]\ttrain-auc:0.99698\teval-auc:0.69558\n",
      "[600]\ttrain-auc:0.99881\teval-auc:0.69585\n",
      "[700]\ttrain-auc:0.99948\teval-auc:0.69054\n",
      "[798]\ttrain-auc:0.99979\teval-auc:0.69132\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.708773559348272\n",
      "[0.708773559348272]\n",
      "[0]\ttrain-auc:0.78154\teval-auc:0.62332\n",
      "[100]\ttrain-auc:0.94122\teval-auc:0.69635\n",
      "[200]\ttrain-auc:0.96977\teval-auc:0.70281\n",
      "[300]\ttrain-auc:0.98674\teval-auc:0.70777\n",
      "[400]\ttrain-auc:0.99446\teval-auc:0.70444\n",
      "[500]\ttrain-auc:0.99774\teval-auc:0.70080\n",
      "[600]\ttrain-auc:0.99911\teval-auc:0.69658\n",
      "[700]\ttrain-auc:0.99967\teval-auc:0.69395\n",
      "[800]\ttrain-auc:0.99989\teval-auc:0.69047\n",
      "[900]\ttrain-auc:0.99998\teval-auc:0.68710\n",
      "[929]\ttrain-auc:0.99999\teval-auc:0.68772\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7093927783582955\n",
      "[0.708773559348272, 0.7093927783582955]\n",
      "[0]\ttrain-auc:0.78427\teval-auc:0.61850\n",
      "[100]\ttrain-auc:0.93736\teval-auc:0.62909\n",
      "[200]\ttrain-auc:0.96596\teval-auc:0.62487\n",
      "[300]\ttrain-auc:0.98210\teval-auc:0.62831\n",
      "[400]\ttrain-auc:0.99080\teval-auc:0.62526\n",
      "[500]\ttrain-auc:0.99560\teval-auc:0.62634\n",
      "[600]\ttrain-auc:0.99805\teval-auc:0.62680\n",
      "[700]\ttrain-auc:0.99917\teval-auc:0.62595\n",
      "[800]\ttrain-auc:0.99965\teval-auc:0.62290\n",
      "[838]\ttrain-auc:0.99974\teval-auc:0.62255\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.6313324819071946\n",
      "[0.708773559348272, 0.7093927783582955, 0.6313324819071946]\n",
      "[0]\ttrain-auc:0.78748\teval-auc:0.58255\n",
      "[100]\ttrain-auc:0.93782\teval-auc:0.68087\n",
      "[200]\ttrain-auc:0.96501\teval-auc:0.69190\n",
      "[300]\ttrain-auc:0.98409\teval-auc:0.69972\n",
      "[400]\ttrain-auc:0.99298\teval-auc:0.70370\n",
      "[500]\ttrain-auc:0.99713\teval-auc:0.70521\n",
      "[600]\ttrain-auc:0.99887\teval-auc:0.70943\n",
      "[700]\ttrain-auc:0.99962\teval-auc:0.70800\n",
      "[800]\ttrain-auc:0.99988\teval-auc:0.70773\n",
      "[900]\ttrain-auc:0.99997\teval-auc:0.70854\n",
      "[999]\ttrain-auc:0.99999\teval-auc:0.70831\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7098184914276867\n",
      "[0.708773559348272, 0.7093927783582955, 0.6313324819071946, 0.7098184914276867]\n",
      "[0]\ttrain-auc:0.77514\teval-auc:0.69660\n",
      "[100]\ttrain-auc:0.93695\teval-auc:0.71504\n",
      "[200]\ttrain-auc:0.96621\teval-auc:0.72255\n",
      "[300]\ttrain-auc:0.98405\teval-auc:0.72259\n",
      "[400]\ttrain-auc:0.99324\teval-auc:0.72263\n",
      "[500]\ttrain-auc:0.99696\teval-auc:0.71907\n",
      "[600]\ttrain-auc:0.99886\teval-auc:0.71915\n",
      "[700]\ttrain-auc:0.99968\teval-auc:0.72422\n",
      "[800]\ttrain-auc:0.99988\teval-auc:0.72452\n",
      "[900]\ttrain-auc:0.99997\teval-auc:0.72576\n",
      "[999]\ttrain-auc:0.99999\teval-auc:0.72360\n",
      "train_auc:<function roc_auc_score at 0x7f91bd897670>,valid_auc0.7260729904408065\n",
      "[0.708773559348272, 0.7093927783582955, 0.6313324819071946, 0.7098184914276867, 0.7260729904408065]\n",
      "all_auc: 0.5\n",
      "OOF-MEAN-AUC:0.697078, OOF-STD-AUC:0.033508\n",
      "Feature\n",
      "n_feat_skew                           545.0\n",
      "interestRate                          500.0\n",
      "avg_income                            463.8\n",
      "dti                                   461.2\n",
      "rest_Revol                            441.2\n",
      "total_income                          431.6\n",
      "regionCode_target_std                 430.6\n",
      "rest_money_rate                       427.8\n",
      "regionCode_isDefault_kfold_mean       422.8\n",
      "rest_money                            421.8\n",
      "revolUtil                             404.0\n",
      "n_feat_std                            403.4\n",
      "all_installment                       381.2\n",
      "issueDate_day                         373.0\n",
      "installment                           373.0\n",
      "regionCode_pred_0                     369.6\n",
      "revolBal                              364.0\n",
      "earliesCreditLine_Allmonth            363.0\n",
      "regionCode_target_skew                361.4\n",
      "n_feat_mean                           354.2\n",
      "mean_interestRate                     345.2\n",
      "annualIncome                          335.8\n",
      "ficoRange_mean                        321.4\n",
      "postCode_count                        316.2\n",
      "earliesCreditLine_month               315.2\n",
      "title_target_skew                     314.8\n",
      "postCode_target_std                   309.4\n",
      "regionCode_count                      294.6\n",
      "postCode_pred_0                       286.6\n",
      "closeAcc                              274.2\n",
      "avg_loanAmnt                          263.4\n",
      "n_feat_sum                            254.4\n",
      "postCode_target_skew                  234.8\n",
      "homeOwnership_pred_0                  234.2\n",
      "title_isDefault_kfold_mean            233.2\n",
      "subGrade                              230.8\n",
      "title_pred_0                          228.6\n",
      "regionCode_pred_1                     227.6\n",
      "loanAmnt                              226.8\n",
      "postCode_isDefault_kfold_mean         224.2\n",
      "issueDate_month                       220.4\n",
      "homeOwnership_isDefault_kfold_mean    218.6\n",
      "totalAcc                              217.8\n",
      "issueDate_month_count                 210.8\n",
      "n6                                    208.0\n",
      "title_target_std                      206.6\n",
      "postCode_pred_1                       200.2\n",
      "dis_time                              198.6\n",
      "earliesCreditLine_year                187.0\n",
      "n_feat_max                            182.6\n",
      "Name: importance, dtype: float64\n",
      "整体指标得分： 0.6826100081272495\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    DATA_PATH ='/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/'\n",
    "    print('读取数据...')\n",
    "    data, train_label = data_preprocess(DATA_PATH=DATA_PATH)\n",
    "\n",
    "    print('开始特征工程...')\n",
    "    data = gen_basicFea(data)\n",
    "\n",
    "\n",
    "    print('data.shape', data.shape)\n",
    "    print('开始模型训练...')\n",
    "    train = data[~data['isDefault'].isnull()].copy()\n",
    "    target = train_label\n",
    "    test = data[~data['isDefault'].isnull()].copy()\n",
    "\n",
    "    target_encode_cols = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "\n",
    "    kflod_num = 5\n",
    "    ss = 0.8\n",
    "    fs = 0.4\n",
    "\n",
    "    class_list = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "    MeanEnocodeFeature = class_list  # 声明需要平均数编码的特征\n",
    "    ME = MeanEncoder(MeanEnocodeFeature, target_type='classification')  # 声明平均数编码的类\n",
    "    train = ME.fit_transform(train, target)  # 对训练数据集的X和y进行拟合\n",
    "    # x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "    test = ME.transform(test)  # 对测试集进行编码\n",
    "    print('num0:mean_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train, test = kfold_stats_feature(train, test, target_encode_cols, kflod_num)\n",
    "    print('num1:target_encode train.shape', train.shape, test.shape)\n",
    "    ### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "    enc_cols = []\n",
    "    stats_default_dict = {\n",
    "        'max': train['isDefault'].max(),\n",
    "        'min': train['isDefault'].min(),\n",
    "        'median': train['isDefault'].median(),\n",
    "        'mean': train['isDefault'].mean(),\n",
    "        'sum': train['isDefault'].sum(),\n",
    "        'std': train['isDefault'].std(),\n",
    "        'skew': train['isDefault'].skew(),\n",
    "        'kurt': train['isDefault'].kurt(),\n",
    "        'mad': train['isDefault'].mad()\n",
    "    }\n",
    "    ### 暂且选择这三种编码\n",
    "    enc_stats = ['max', 'min', 'skew', 'std']\n",
    "    skf = KFold(n_splits=kflod_num, shuffle=True, random_state=6666)\n",
    "    for f in tqdm(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']):\n",
    "        enc_dict = {}\n",
    "        for stat in enc_stats:\n",
    "            enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "            train['{}_target_{}'.format(f, stat)] = 0\n",
    "            test['{}_target_{}'.format(f, stat)] = 0\n",
    "            enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "        for i, (trn_idx, val_idx) in enumerate(skf.split(train, target)):\n",
    "            trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "            enc_df = trn_x.groupby(f, as_index=False)['isDefault'].agg(enc_dict)\n",
    "            val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "            test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "            for stat in enc_stats:\n",
    "                val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "                test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits\n",
    "\n",
    "    print('num2:target_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "    test.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "    print('输入数据维度：', train.shape, test.shape)\n",
    "    \n",
    "    xgb_preds, xgb_oof, xgb_score, feaNum = xgb_model(train=train, target=target, test=test, k=kflod_num)\n",
    "\n",
    "    lgb_score = round(xgb_score, 5)\n",
    "    sub_df = test[['id']].copy()\n",
    "    sub_df['isDefault'] = xgb_preds\n",
    "    off = test[['id']].copy()\n",
    "    subVal_df = train[['id']].copy()\n",
    "    subVal_df['isDefault'] = xgb_oof\n",
    "    outpath = '/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/'\n",
    "\n",
    "    all_auc_score = roc_auc_score(train_label, subVal_df['isDefault'])\n",
    "    print('整体指标得分：', all_auc_score)\n",
    "    all_auc_score = round(all_auc_score, 5)\n",
    "\n",
    "    sub_df.to_csv(outpath+'xgb1.csv',index=False)\n",
    "    subVal_df.to_csv(outpath+'xgb1Val.csv',index=False)\n",
    "    sub_df.to_csv(\n",
    "         outpath + str(all_auc_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_{}_xgb.csv'.format(ss, fs,\n",
    "                                                                                                       kflod_num),\n",
    "         index=False)\n",
    "    subVal_df.to_csv(\n",
    "         outpath + str(all_auc_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_{}_subVal.csv'.format(ss, fs,\n",
    "                                                                                                          kflod_num),\n",
    "         index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "suited-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test 含有target的值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-breast",
   "metadata": {},
   "source": [
    "train = train.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
    "test = test.apply(lambda x: pd.to_numeric(x,errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "diagnostic-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =train[~train.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "# powerful tools 使用skearn会对dataset有特定的要求，当出现nan，inf时，可以用这行代码进行数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "appreciated-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.drop(['isDefault'],axis=1)\n",
    "test1 = test.drop(['isDefault'],axis =1)\n",
    "target = train['isDefault']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "classified-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(train1,target,test_size = 0.2,random_state=6666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "atlantic-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic \n",
    "# import LogisticRegression model in python. \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "caring-equity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7277836318932209"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## call on the model object\n",
    "logreg = LogisticRegression(solver='newton-cg',\n",
    "                           penalty= 'l2',random_state = 6666,\n",
    "                            class_weight={0:0.25, 1:0.75},\n",
    "                            warm_start=True,\n",
    "                            max_iter =1000,\n",
    "                            C = 0.4\n",
    "                           )\n",
    "\n",
    "\n",
    "## fit the model with \"train_x\" and \"train_y\"\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "## Once the model is trained we want to find out how well the model is performing, so we test the model. \n",
    "## we use \"X_test\" portion of the data(this data was not used to fit the model) to predict model outcome. \n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "understood-magnitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1\n",
      "0  1776  494\n",
      "1   281  296\n",
      "0.7188824162652028\n"
     ]
    }
   ],
   "source": [
    "# printing confision matrix and auc\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_pred)))\n",
    "y_score = logreg.decision_function(X_test)\n",
    "FPR, TPR, _ = roc_curve(y_test, y_score)\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "print (ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "printable-testimony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=30, random_state=None, test_size=0.25,\n",
       "            train_size=None),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.45, 0.4, 0.5,\n",
       "                               0.6, 0.7, 0.8, 0.9, 1]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "# C_vals is the alpla value of lasso and ridge regression(as \n",
    "# alpha increases the model complexity decreases,)\n",
    "\n",
    "## remember effective alpha scores are 0<alpha<infinity \n",
    "C_vals = [0.1,0.15,0.2,0.25,0.3,0.35,0.45,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "\n",
    "## Choosing penalties(Lasso(l1) or Ridge(l2))\n",
    "# penalties = ['l1','l2']\n",
    "\n",
    "## Choose a cross validation strategy. \n",
    "cv = StratifiedShuffleSplit(n_splits = 30, test_size = .25)\n",
    "\n",
    "## setting param for param_grid in GridSearchCV. \n",
    "param = {'C': C_vals}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "## Calling on GridSearchCV object. \n",
    "grid = GridSearchCV(\n",
    "    estimator=LogisticRegression(), \n",
    "    param_grid = param,\n",
    "    #scoring = 'accuracy',\n",
    "    scoring = 'roc_auc',\n",
    "    n_jobs =-1,\n",
    "    cv = cv\n",
    ")\n",
    "## Fitting the model\n",
    "grid.fit(train1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "senior-boston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6182815165473571\n",
      "{'C': 0.9}\n",
      "LogisticRegression(C=0.9)\n"
     ]
    }
   ],
   "source": [
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "### amzing, I am going to try KNN rightnow!! but Knn is not nn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "potential-strand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52799404, 0.53588555, 0.5163963 , 0.52606544, 0.50131433])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Importing the model. \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "## calling on the model oject. \n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
    "                                            metric='minkowski',\n",
    "                                            metric_params=None, n_jobs=-1,\n",
    "                                            n_neighbors=29, p=2,\n",
    "                                            weights='uniform')\n",
    "## knn classifier works by doing euclidian distance \n",
    "\n",
    "\n",
    "## doing 10 fold staratified-shuffle-split cross validation \n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=.33, random_state=666)\n",
    "\n",
    "accuracies = cross_val_score(knn, train1,target,scoring='roc_auc')\n",
    "accuracies\n",
    "#accuracies = cross_val_score(knn, train1,target,scoring='roc_auc' #cv = cv )\n",
    "#print(\"Cross-Validation accuracy scores:{}\".format(accuracies))\n",
    "#print(\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sublime-gauge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8003514938488576\n",
      "0.6876176059618071\n"
     ]
    }
   ],
   "source": [
    "knnfit = knn.fit(X_train,y_train)\n",
    "y_prediction = knnfit.predict(X_test)\n",
    "print(accuracy_score(y_test,y_prediction))\n",
    "FPR, TPR, _ = roc_curve(y_test, y_score)\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "print (ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "backed-rachel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=0.25,\n",
       "            train_size=None),\n",
       "                   estimator=KNeighborsClassifier(), n_iter=40, n_jobs=-1,\n",
       "                   param_distributions={'n_neighbors': range(1, 31)},\n",
       "                   verbose=False)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#两种search调整参数\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# C_vals is the alpla value of lasso and ridge regression(as \n",
    "# alpha increases the model complexity decreases,)\n",
    "\n",
    "## remember effective alpha scores are 0<alpha<infinity \n",
    "k_range = range(1,31)\n",
    "\n",
    "## Choosing penalties(Lasso(l1) or Ridge(l2))\n",
    "# penalties = ['l1','l2']\n",
    "\n",
    "## Choose a cross validation strategy. \n",
    "cv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n",
    "\n",
    "## setting param for param_grid in GridSearchCV. \n",
    "param = {'n_neighbors': k_range}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "## Calling on GridSearchCV object. \n",
    "#grid = GridSearchCV(\n",
    "#    estimator= KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
    "#                                            metric='minkowski',\n",
    "#                                            metric_params=None, n_jobs=None,\n",
    "#                                            n_neighbors=5, p=2,\n",
    "#                                            weights='uniform') or 之前定义的knn, \n",
    "#    param_grid = param,\n",
    "#    #scoring = 'accuracy',\n",
    "#    scoring = 'roc_auc',\n",
    "#    n_jobs =-1,\n",
    "#    cv = cv\n",
    "grid = RandomizedSearchCV(knn, param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\n",
    "\n",
    "## Fitting the model\n",
    "grid.fit(train1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "british-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799775344004493\n",
      "{'n_neighbors': 30}\n",
      "KNeighborsClassifier(n_neighbors=30)\n"
     ]
    }
   ],
   "source": [
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "clean-going",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7997753440044931"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid= grid.best_estimator_\n",
    "knn_grid.score(train1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am not sure why cross_val_score is lower than score that fit by myself!!!! but great try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "introductory-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "###贝叶斯定律"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "scientific-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.362\n",
      "0.7202872161027591\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train,y_train)\n",
    "y_pred = gaussian.predict(X_test)\n",
    "gaussian_accy = round(accuracy_score(y_pred, y_test), 3)\n",
    "FPR, TPR, _ = roc_curve(y_test, y_score)\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "print(gaussian_accy)\n",
    "print(ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "plastic-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "####今天学了如何在别人代码基础上做数据调整、如何调用各种统计模型、如何利用gridsearch来寻找最优参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "###支持向量机-我要做最优秀的解剖手-可分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "Cs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \n",
    "gammas = [0.0001,0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n",
    "grid_search = tqdm(GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv)) ## 'rbf' stands for gaussian kernel\n",
    "grid_search.fit(train1,target)\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "nervous-penalty",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-212bef4d72ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msvc_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel = 'rbf', probability=True, C = 0.01, gamma = 0.1,cache_size=2000,degree=3, class_weight = None, max_iter=-1)\n",
    "svc_reg = svc.fit(X_train, y_train)\n",
    "y_pre = svc_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "tracked-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7973305233579207\n",
      "0.7188824162652028\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pre))\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "print(ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "concrete-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "imperial-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(class_weight='balanced',max_depth=100, max_features=100,\n",
    "                                              criterion='gini', \n",
    "                                              max_leaf_nodes=3,\n",
    "                                              min_impurity_decrease=1.0,\n",
    "                                              min_impurity_split=None,\n",
    "                                              min_samples_leaf=4,\n",
    "                                              min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,\n",
    "                                              random_state=666,\n",
    "                                              splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "theoretical-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=20, random_state=15, test_size=None,\n",
       "            train_size=None),\n",
       "             estimator=DecisionTreeClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['entropy', 'gini'],\n",
       "                         'max_depth': range(1, 30),\n",
       "                         'max_features': [21, 22, 23, 24, 25, 26, 28, 29, 30,\n",
       "                                          'auto']},\n",
       "             verbose=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "max_depth = range(1,30)\n",
    "max_feature = [21,22,23,24,25,26,28,29,30,'auto']\n",
    "criterion=[\"entropy\", \"gini\"]\n",
    "\n",
    "param = {'max_depth':max_depth, \n",
    "         'max_features':max_feature, \n",
    "         'criterion': criterion}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), \n",
    "                                param_grid = param, \n",
    "                                 verbose=False, \n",
    "                                 cv=StratifiedShuffleSplit(n_splits=20, random_state=15),\n",
    "                                n_jobs = -1)\n",
    "grid.fit(train1, target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "right-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8028089887640452\n",
      "{'criterion': 'entropy', 'max_depth': 3, 'max_features': 22}\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=3, max_features=22)\n"
     ]
    }
   ],
   "source": [
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "photographic-african",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026981450252951"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dectree_grid = grid.best_estimator_\n",
    "## using the best found hyper paremeters to get the score. \n",
    "dectree_grid.score(train1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "other-wagner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subGrade</th>\n",
       "      <td>0.475293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_interestRate</th>\n",
       "      <td>0.367313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dti</th>\n",
       "      <td>0.079010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_installment</th>\n",
       "      <td>0.043798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rest_Revol</th>\n",
       "      <td>0.034586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closeAcc</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postCode_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_pred_0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employmentTitle_pred_1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employmentTitle_pred_0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        importance\n",
       "subGrade                  0.475293\n",
       "mean_interestRate         0.367313\n",
       "dti                       0.079010\n",
       "all_installment           0.043798\n",
       "rest_Revol                0.034586\n",
       "closeAcc                  0.000000\n",
       "postCode_count            0.000000\n",
       "title_pred_0              0.000000\n",
       "employmentTitle_pred_1    0.000000\n",
       "employmentTitle_pred_0    0.000000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = train1.columns\n",
    "feature_importances = pd.DataFrame(dectree_grid.feature_importances_,\n",
    "                                   index = column_names,\n",
    "                                    columns=['importance'])\n",
    "feature_importances.sort_values(by='importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "golden-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "##我打算停到这了，因为我暂时定义不到问题了。那个统计科学家的方法在这里可以跑通，剩下的是一些体力活。而我想学一些自己还不理解的东西，比如特征工程。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
