{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "parliamentary-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-15\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import entropy\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import datetime\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "nowtime = datetime.date.today()\n",
    "nowtime = str(nowtime)[-5:]\n",
    "print(nowtime)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/'\n",
    "\n",
    "def load_dataset(DATA_PATH):\n",
    "    train_label = pd.read_csv(DATA_PATH + 'train.csv')['isDefault']\n",
    "    train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "    test = pd.read_csv(DATA_PATH + 'testA.csv')\n",
    "#     train = train.sample(40000)\n",
    "#     test = test.sample(5000)\n",
    "#     train.to_csv(DATA_PATH + 'target.csv')\n",
    "#     train_label = pd.read_csv(DATA_PATH + 'target.csv')['isDefault']\n",
    "    \n",
    "    feats = [f for f in train.columns if f not in ['isDefault']]\n",
    "    # train = train[feats]\n",
    "    test = test[feats]\n",
    "    print('train.shape', train.shape)\n",
    "    print('test.shape', test.shape)\n",
    "\n",
    "    return train_label, train, test\n",
    "\n",
    "\n",
    "# 处理时间\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400 * day + 3600 * hour + 60 * minute + second\n",
    "\n",
    "\n",
    "def transform_day(date1):\n",
    "    date2 = \"2020-01-01\"\n",
    "    date1 = time.strptime(date1, \"%Y-%m-%d\")\n",
    "    date2 = time.strptime(date2, \"%Y-%m-%d\")\n",
    "\n",
    "    # 根据上面需要计算日期还是日期时间，来确定需要几个数组段。下标0表示年，小标1表示月，依次类推...\n",
    "    # date1=datetime.datetime(date1[0],date1[1],date1[2],date1[3],date1[4],date1[5])\n",
    "    # date2=datetime.datetime(date2[0],date2[1],date2[2],date2[3],date2[4],date2[5])\n",
    "    date1 = datetime.datetime(date1[0], date1[1], date1[2])\n",
    "    date2 = datetime.datetime(date2[0], date2[1], date2[2])\n",
    "    # 返回两个变量相差的值，就是相差天数\n",
    "    # print((date2 - date1).days)  # 将天数转成int型\n",
    "    return (date2 - date1).days\n",
    "\n",
    "\n",
    "# transform_day('2007-09-01')\n",
    "\n",
    "def labelEncoder_df(df, features):\n",
    "    for i in features:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        df[i] = encoder.fit_transform(df[i])\n",
    "\n",
    "\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg([('mean','mean'), ('beta','size')])\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target,\n",
    "                        self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.index.get_loc[large_ind], y.index.get_loc[large_ind], X_new.index.get_loc[small_ind], variable, None,\n",
    "                        self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "\n",
    "def employmentLength_deal(x):\n",
    "    if x == r'\\N':\n",
    "        result = -999\n",
    "    elif x == -999:\n",
    "        result = -999\n",
    "    elif x == '-999':\n",
    "        result = -999\n",
    "    elif x == '< 1 year':\n",
    "        result = 0.5\n",
    "    elif x == '10+ years':\n",
    "        result = 12\n",
    "    else:\n",
    "        result = int(x.split(' ')[0][0])\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def earliesCreditLine_month_deal(x):\n",
    "    x = x.split('-')[0]\n",
    "    # print(x)\n",
    "    dict = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10,\n",
    "            'Nov': 11, 'Dec': 12}\n",
    "    result = dict[x]\n",
    "    return result\n",
    "\n",
    "\n",
    "def gradeTrans(x):\n",
    "    dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    result = dict[x]\n",
    "    return result\n",
    "\n",
    "\n",
    "def subGradeTrans(x):\n",
    "    dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    result = dict[x[0]]\n",
    "    result = result * 5 + int(x[1])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def myEntro(x):\n",
    "    \"\"\"\n",
    "        calculate shanno ent of x\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        p = float(x[x == x_value].shape[0]) / x.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "    #     print(x_value,p,logp)\n",
    "    # print(ent)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def myRms(records):\n",
    "    records = list(records)\n",
    "    \"\"\"\n",
    "    均方根值 反映的是有效值而不是平均值\n",
    "    \"\"\"\n",
    "    return np.math.sqrt(sum([x ** 2 for x in records]) / len(records))\n",
    "\n",
    "\n",
    "def myMode(x):\n",
    "    return np.mean(pd.Series.mode(x))\n",
    "\n",
    "\n",
    "def myQ25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "\n",
    "def myQ75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "\n",
    "def myRange(x):\n",
    "    return pd.Series.max(x) - pd.Series.min(x)\n",
    "\n",
    "\n",
    "# 预处理\n",
    "def data_preprocess(DATA_PATH):\n",
    "    train_label, train, test = load_dataset(DATA_PATH=DATA_PATH)\n",
    "    # 拼接数据\n",
    "\n",
    "    data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    print('初始拼接后：', data.shape)\n",
    "    # n_feat = [f for f in data.columns if f[0] == 'n']\n",
    "\n",
    "    n_feat = ['n0', 'n1', 'n2', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14', ]\n",
    "    # nameList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', 'mode', 'range']\n",
    "    # statList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', myMode, myRange]\n",
    "    nameList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "    statList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "\n",
    "\n",
    "    for i in range(len(nameList)):\n",
    "        data['n_feat_{}'.format(nameList[i])] = data[n_feat].agg(statList[i], axis=1)\n",
    "    print('n特征处理后：', data.shape)\n",
    "\n",
    "\n",
    "    # count编码\n",
    "    count_list = ['subGrade', 'grade', 'postCode', 'regionCode','homeOwnership','title','employmentTitle','employmentLength']\n",
    "    data = count_coding(data, count_list)\n",
    "    print('count编码后：', data.shape)\n",
    "    ### 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "    cross_cat = ['subGrade', 'grade', 'employmentLength', 'term', 'homeOwnership', 'postCode', 'regionCode','employmentTitle','title']\n",
    "    cross_num = ['dti', 'revolBal','revolUtil', 'ficoRangeHigh', 'interestRate', 'loanAmnt', 'installment', 'annualIncome', 'n14',\n",
    "                 'n2', 'n6', 'n9', 'n5', 'n8']\n",
    "\n",
    "    data[['employmentLength']].fillna(-999, inplace=True)\n",
    "\n",
    "#     data = cross_cat_num(data, cross_num, cross_cat)  # 一阶交叉\n",
    "#     print('一阶特征处理后：', data.shape)\n",
    "#     data = cross_qua_cat_num(data)  # 二阶交叉\n",
    "#     print('二阶特征处理后：', data.shape)\n",
    "    # 缺失值处理\n",
    "    for temp in count_list:\n",
    "        del data[temp+'_count']\n",
    "    # num_fill_col = ['employmentLength', 'postCode', ]\n",
    "    cols = ['employmentTitle', 'employmentLength', 'postCode', 'dti', 'pubRecBankruptcies', 'revolUtil', 'title',\n",
    "            'n0', 'n1', 'n2', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14']\n",
    "    for col in cols:\n",
    "        data[col].fillna(r'\\N', inplace=True)\n",
    "    cols = [f for f in cols if f not in ['employmentLength']]\n",
    "    for col in cols:\n",
    "        data[col].replace({r'\\N': -999}, inplace=True)\n",
    "        data[col] = data[col]\n",
    "    # print('缺失值情况：', data.isnull().sum())\n",
    "\n",
    "    data['grade'] = data['grade'].apply(lambda x: gradeTrans(x))\n",
    "    data['subGrade'] = data['subGrade'].apply(lambda x: subGradeTrans(x))\n",
    "    print('1data.shape', data.shape)\n",
    "\n",
    "    data['employmentLength'] = data['employmentLength'].apply(lambda x: employmentLength_deal(x))\n",
    "    data['issueDate_year'] = data['issueDate'].apply(lambda x: int(x.split('-')[0]))\n",
    "    data['issueDate_month'] = data['issueDate'].apply(lambda x: int(x.split('-')[1]))\n",
    "    data['issueDate_day'] = data['issueDate'].apply(lambda x: transform_day(x))\n",
    "    data['issueDate_week'] = data['issueDate_day'].apply(lambda x: int(x % 7) + 1)\n",
    "\n",
    "    print('2_data.shape', data.shape)\n",
    "    data['earliesCreditLine_year'] = data['earliesCreditLine'].apply(lambda x: 2020 - (int(x.split('-')[-1])))\n",
    "    data['earliesCreditLine_month'] = data['earliesCreditLine'].apply(lambda x: earliesCreditLine_month_deal(x))\n",
    "    data['earliesCreditLine_Allmonth'] = data['earliesCreditLine_year'] * 12 - data['earliesCreditLine_month']\n",
    "    del data['issueDate'], data['earliesCreditLine']\n",
    "\n",
    "    print('预处理完毕', data.shape)\n",
    "\n",
    "    return data, train_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kfold_stats_feature(train, test, feats, k):\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=6666)  # 这里最好和后面模型的K折交叉验证保持一致\n",
    "\n",
    "    train['fold'] = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['isDefault'])):\n",
    "        train.loc[val_idx, 'fold'] = fold_\n",
    "\n",
    "    kfold_features = []\n",
    "    for feat in feats:\n",
    "        nums_columns = ['isDefault']\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            kfold_features.append(colname)\n",
    "            train[colname] = None\n",
    "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['isDefault'])):\n",
    "                tmp_trn = train.iloc[trn_idx]\n",
    "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
    "                tmp = train.loc[train.fold == fold_, [feat]]\n",
    "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n",
    "                # fillna\n",
    "                global_mean = train[f].mean()\n",
    "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
    "            train[colname] = train[colname].astype(float)\n",
    "\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            test[colname] = None\n",
    "            order_label = train.groupby([feat])[f].mean()\n",
    "            test[colname] = test[feat].map(order_label)\n",
    "            # fillna\n",
    "            global_mean = train[f].mean()\n",
    "            test[colname] = test[colname].fillna(global_mean)\n",
    "            test[colname] = test[colname].astype(float)\n",
    "    del train['fold']\n",
    "    return train, test\n",
    "\n",
    "def GridSearch(clf, params, X, y):\n",
    "    cscv = GridSearchCV(clf, params, scoring='roc_auc', n_jobs=4, cv=10)\n",
    "    cscv.fit(X, y)\n",
    "    print(cscv.cv_results_)\n",
    "    print(cscv.best_params_)\n",
    "    print(cscv.best_score_)\n",
    "\n",
    "### count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return (df)\n",
    "\n",
    "\n",
    "# 定义交叉特征统计\n",
    "def cross_cat_num(df, num_col, cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return (df)\n",
    "\n",
    "\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['subGrade', 'regionCode'], ['grade', 'regionCode'], ['subGrade', 'postCode'], ['grade', 'postCode'], ['employmentTitle','title'],\n",
    "        ['regionCode','title'], ['postCode','title'], ['homeOwnership','title'], ['homeOwnership','employmentTitle'],['homeOwnership','employmentLength'],\n",
    "        ['regionCode', 'postCode']\n",
    "    ]):\n",
    "        ### 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['id'].transform('count')\n",
    "        ### n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        ### 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return (df)\n",
    "\n",
    "\n",
    "### count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return (df)\n",
    "\n",
    "def gen_basicFea(data):\n",
    "    data['avg_income'] = data['annualIncome'] / data['employmentLength']\n",
    "    data['total_income'] = data['annualIncome'] * data['employmentLength']\n",
    "    data['avg_loanAmnt'] = data['loanAmnt'] / data['term']\n",
    "    data['mean_interestRate'] = data['interestRate'] / data['term']\n",
    "    data['all_installment'] = data['installment'] * data['term']\n",
    "\n",
    "    data['rest_money_rate'] = data['avg_loanAmnt'] / (data['annualIncome'] + 0.1)  # 287个收入为0\n",
    "    data['rest_money'] = data['annualIncome'] - data['avg_loanAmnt']\n",
    "\n",
    "    data['closeAcc'] = data['totalAcc'] - data['openAcc']\n",
    "    data['ficoRange_mean'] = (data['ficoRangeHigh'] + data['ficoRangeLow']) / 2\n",
    "    del data['ficoRangeHigh'], data['ficoRangeLow']\n",
    "\n",
    "    data['rest_pubRec'] = data['pubRec'] - data['pubRecBankruptcies']\n",
    "\n",
    "    data['rest_Revol'] = data['loanAmnt'] - data['revolBal']\n",
    "\n",
    "    data['dis_time'] = data['issueDate_year'] - (2020 - data['earliesCreditLine_year'])\n",
    "    for col in ['employmentTitle', 'grade', 'subGrade', 'regionCode', 'issueDate_month', 'postCode']:\n",
    "        data['{}_count'.format(col)] = data.groupby([col])['id'].transform('count')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def plotroc(train_y, train_pred, test_y, val_pred):\n",
    "    lw = 2\n",
    "    ##train\n",
    "    fpr, tpr, thresholds = roc_curve(train_y.values, train_pred, pos_label=1.0)\n",
    "    train_auc_value = roc_auc_score(train_y.values, train_pred)\n",
    "    ##valid\n",
    "    fpr, tpr, thresholds = roc_curve(test_y.values, val_pred, pos_label=1.0)\n",
    "    valid_auc_value = roc_auc_score(test_y.values, val_pred)\n",
    "\n",
    "    return train_auc_value, valid_auc_value\n",
    "\n",
    "\n",
    "def xgb_model(train, target, test, k):\n",
    "    # saveFeature_df = pd.read_csv('../feature/xgb_920_556_5_score.csv',header=None)\n",
    "#     saveFeature_df2 = pd.read_csv('../feature/xgb_09-20_74_0.8_0.6_5_score.csv',header=None)\n",
    "    \n",
    "    # saveFeature_df.columns=['feature','score']\n",
    "#     saveFeature_df2.columns=['feature','score']\n",
    "\n",
    "#     saveFeature_list = list(saveFeature_df['feature'].values)\n",
    "#     saveFeature_list2 = list(saveFeature_df2['feature'].values)\n",
    "    \n",
    "    # saveFeature_list = list(saveFeature_df[saveFeature_df['score']>10]['feature'])\n",
    "    saveFeature_list=list(train.columns)\n",
    "    feats = [f for f in saveFeature_list if f not in ['id', 'isDefault']]\n",
    "    feaNum = len(feats)\n",
    "    print('Current num of features:', len(feats))\n",
    "\n",
    "    seeds = [2020,666666,188888]\n",
    "    output_preds = 0\n",
    "    xgb_oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "        offline_score = []\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        params = {'booster': 'gbtree',\n",
    "                  'objective': 'binary:logistic',\n",
    "                  'eval_metric': 'auc',\n",
    "                  'min_child_weight': 5,\n",
    "                  'max_depth': 8,\n",
    "                  'subsample': ss,\n",
    "                  'colsample_bytree': fs,\n",
    "                  'eta': 0.01,\n",
    "\n",
    "                  'seed': seed,\n",
    "                  'nthread': -1,\n",
    "\n",
    "                  'tree_method': 'hist'\n",
    "                  }\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            \n",
    "            train_y, test_y = target[train_index], target[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            train_matrix = xgb.DMatrix(train_X, label=train_y, missing=np.nan)\n",
    "            valid_matrix = xgb.DMatrix(test_X, label=test_y, missing=np.nan)\n",
    "            test_matrix = xgb.DMatrix(test[feats], missing=np.nan)\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = xgb.train(params, train_matrix, num_boost_round=1000, evals=watchlist, verbose_eval=100,\n",
    "                              early_stopping_rounds=600)\n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            train_pred = model.predict(train_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            xgb_oof_probs[test_index] += val_pred / len(seeds)\n",
    "            # oof_probs[test_index] += val_pred\n",
    "            test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "            # 绘制roc曲线\n",
    "            train_auc_value, valid_auc_value = plotroc(train_y, train_pred, test_y, val_pred)\n",
    "            print('train_auc:{},valid_auc{}'.format(train_auc_value, valid_auc_value))\n",
    "            offline_score.append(valid_auc_value)\n",
    "            print(offline_score)\n",
    "            output_preds += test_pred / k / len(seeds)\n",
    "            \n",
    "#             sub_df = test[['id']].copy()\n",
    "#             sub_df['isDefault'] = output_preds\n",
    "#             off = test[['id']].copy()\n",
    "#             subVal_df = train[['id']].copy()\n",
    "#             subVal_df.loc[test_index,'isDefault'] = xgb_oof_probs[test_index]\n",
    "#             outpath = '../user_data/fold/'\n",
    "#             fold_score = round(valid_auc_value, 5)\n",
    "#             sub_df.to_csv( outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgb.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "#             subVal_df.to_csv(outpath + str(fold_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_xgbVal.csv'.format(i, kflod_num),\n",
    "#             index=False)\n",
    "\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"Feature\"] = model.get_fscore().keys()\n",
    "            fold_importance_df[\"importance\"] = model.get_fscore().values()\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "#             print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "#             feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "#             feature_sorted.to_csv('../feature/xgb_{}_{}_{}_{}_score.csv'.format(i+1,nowtime, feaNum, kflod_num))\n",
    "\n",
    "#             if i==3:\n",
    "#                 break\n",
    "#             gc.collect()\n",
    "\n",
    "\n",
    "        print('all_auc:', roc_auc_score(target.values, oof_probs))\n",
    "        print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "        feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "        feature_sorted.to_csv('../feature/xgb_importance.csv')\n",
    "        top_features = feature_sorted.index\n",
    "        print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    return output_preds, xgb_oof_probs, np.mean(offline_score), feaNum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "green-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "train.shape (800000, 47)\n",
      "test.shape (200000, 46)\n",
      "初始拼接后： (1000000, 47)\n",
      "n特征处理后： (1000000, 53)\n",
      "count编码后： (1000000, 61)\n",
      "1data.shape (1000000, 53)\n",
      "2_data.shape (1000000, 57)\n",
      "预处理完毕 (1000000, 58)\n",
      "开始特征工程...\n",
      "data.shape (1000000, 74)\n",
      "开始模型训练...\n",
      "num0:mean_encode train.shape (800000, 84) (200000, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num1:target_encode train.shape (800000, 89) (200000, 89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:41<00:00, 32.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num2:target_encode train.shape (800000, 109) (200000, 109)\n",
      "输入数据维度： (800000, 104) (200000, 104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    DATA_PATH ='/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/'\n",
    "    print('读取数据...')\n",
    "    data, train_label = data_preprocess(DATA_PATH=DATA_PATH)\n",
    "\n",
    "    print('开始特征工程...')\n",
    "    data = gen_basicFea(data)\n",
    "\n",
    "\n",
    "    print('data.shape', data.shape)\n",
    "    print('开始模型训练...')\n",
    "    train = data[~data['isDefault'].isnull()].copy()\n",
    "    target = train_label\n",
    "    test = data[data['isDefault'].isnull()].copy()\n",
    "\n",
    "    target_encode_cols = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "\n",
    "    kflod_num = 5\n",
    "    ss = 0.8\n",
    "    fs = 0.4\n",
    "\n",
    "    class_list = ['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']\n",
    "    MeanEnocodeFeature = class_list  # 声明需要平均数编码的特征\n",
    "    ME = MeanEncoder(MeanEnocodeFeature, target_type='classification')  # 声明平均数编码的类\n",
    "    train = ME.fit_transform(train, target)  # 对训练数据集的X和y进行拟合\n",
    "    # x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "    test = ME.transform(test)  # 对测试集进行编码\n",
    "    print('num0:mean_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train, test = kfold_stats_feature(train, test, target_encode_cols, kflod_num)\n",
    "    print('num1:target_encode train.shape', train.shape, test.shape)\n",
    "    ### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "    enc_cols = []\n",
    "    stats_default_dict = {\n",
    "        'max': train['isDefault'].max(),\n",
    "        'min': train['isDefault'].min(),\n",
    "        'median': train['isDefault'].median(),\n",
    "        'mean': train['isDefault'].mean(),\n",
    "        'sum': train['isDefault'].sum(),\n",
    "        'std': train['isDefault'].std(),\n",
    "        'skew': train['isDefault'].skew(),\n",
    "        'kurt': train['isDefault'].kurt(),\n",
    "        'mad': train['isDefault'].mad()\n",
    "    }\n",
    "    ### 暂且选择这三种编码\n",
    "    enc_stats = ['max', 'min', 'skew', 'std']\n",
    "    skf = KFold(n_splits=kflod_num, shuffle=True, random_state=6666)\n",
    "    for f in tqdm(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title']):\n",
    "        enc_dict = {}\n",
    "        for stat in enc_stats:\n",
    "            enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "            train['{}_target_{}'.format(f, stat)] = 0\n",
    "            test['{}_target_{}'.format(f, stat)] = 0\n",
    "            enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "        for i, (trn_idx, val_idx) in enumerate(skf.split(train, target)):\n",
    "            trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "            enc_df = trn_x.groupby(f, as_index=False)['isDefault'].agg(enc_dict)\n",
    "            val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "            test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "            for stat in enc_stats:\n",
    "                val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "                test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits\n",
    "\n",
    "    print('num2:target_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "    test.drop(['postCode', 'regionCode', 'homeOwnership', 'employmentTitle','title'], axis=1, inplace=True)\n",
    "    print('输入数据维度：', train.shape, test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suited-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test 含有target的值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-breast",
   "metadata": {},
   "source": [
    "train = train.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
    "test = test.apply(lambda x: pd.to_numeric(x,errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "diagnostic-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =train[~train.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "# test =test[~test.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "# powerful tools 使用skearn会对dataset有特定的要求，当出现nan，inf时，可以用这行代码进行数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "hearing-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.drop(['isDefault','id'],axis=1)\n",
    "# test1 = train.drop(['isDefault','id'],axis=1)\n",
    "target = train['isDefault']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "classified-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(train1,target,test_size = 0.33,random_state=6666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "atlantic-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-thinking",
   "metadata": {},
   "source": [
    "FPR, TPR, _ = roc_curve(y_test, y_score)\n",
    "ROC_AUC = auc(FPR, TPR)\n",
    "print (ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caring-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 多元逻辑回归\n",
    "logreg = LogisticRegression(solver='newton-cg',\n",
    "                           penalty= 'l2',random_state = 6666,\n",
    "                            class_weight={0:0.25, 1:0.75},\n",
    "                            warm_start=True,\n",
    "                            max_iter =1000\n",
    "                           )\n",
    "C_vals = [0.1,0.3,0.5,0.7,0.9]\n",
    "cv = StratifiedShuffleSplit(n_splits = 3, test_size = .25)\n",
    "param = {'C': C_vals}\n",
    "grid = GridSearchCV(\n",
    "    estimator=LogisticRegression(), \n",
    "    param_grid = param,\n",
    "#    scoring = 'accuracy',\n",
    "    scoring = 'roc_auc',\n",
    "    n_jobs =-1,\n",
    "    cv = cv\n",
    ")\n",
    "grid.fit(train1, target)\n",
    "grid.best_score_\n",
    "log_grid= grid.best_estimator_\n",
    "# 到这里log_grid 可以用作predict test 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "potential-strand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5932676869013956"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn model \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
    "                                            metric='minkowski',\n",
    "                                            metric_params=None, n_jobs=-1,\n",
    "                                            p=2\n",
    "                                            )\n",
    "k_range = range(1,31)\n",
    "weights_options=['uniform','distance']\n",
    "param = {'n_neighbors':k_range, 'weights':weights_options}\n",
    "cv = StratifiedShuffleSplit(n_splits = 3, test_size = .25)\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param,cv=cv,verbose = False,scoring = 'roc_auc', n_jobs=-1)\n",
    "grid_knn.fit(train1,target)\n",
    "knn_grid = grid_knn.best_estimator_\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "scientific-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gaussian = GaussianNB()\n",
    "gau = gaussian.fit(train1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "boxed-contest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#支持向量机\n",
    "Cs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10]\n",
    "gammas = [0.0001,0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.30, random_state=66)\n",
    "grid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True),param_grid, cv=cv,scoring = 'roc_auc') ## 'rbf' stands for gaussian kernel\n",
    "grid_search.fit(train1,target)\n",
    "grid_svc = grid_search.best_estimator_\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "imperial-award",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-02d218b7635a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                 \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                    )\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgrid_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mgrid_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mgrid_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##Decision tree\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', \n",
    "                                              max_leaf_nodes=3,\n",
    "                                              min_impurity_decrease=1.0,\n",
    "                                              min_impurity_split=None,\n",
    "                                              min_samples_leaf=4,\n",
    "                                              min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,\n",
    "                                              random_state=666,\n",
    "                                              splitter='best')\n",
    "max_depth = range(1,30)\n",
    "max_feature = [21,22,23,24,25,26,28,29,30,'auto']\n",
    "criterion=[\"entropy\", \"gini\"]\n",
    "cv = StratifiedShuffleSplit(n_splits=3, random_state=15)\n",
    "param = {'max_depth':max_depth, \n",
    "         'max_features':max_feature, \n",
    "         'criterion': criterion}\n",
    "grid_d = GridSearchCV(dt, \n",
    "                                param_grid = param, \n",
    "                                 verbose=False, \n",
    "                                 cv=cv,\n",
    "                                n_jobs = -1,scoring = 'roc_auc'\n",
    "                   )\n",
    "grid_d.fit(train1, target) \n",
    "grid_dt = grid_d.best_estimator_\n",
    "grid_d.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "theoretical-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4bb4f1955be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                  \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                  n_jobs = -1,scoring = 'roc_auc')\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mgrid_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mgrid_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mgrid_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_estimators = [140,145,150,155,160]\n",
    "max_depth = range(1,10)\n",
    "criterions = ['gini', 'entropy']\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.30, random_state=15)\n",
    "\n",
    "\n",
    "parameters = {'n_estimators':n_estimators,\n",
    "              'max_depth':max_depth,\n",
    "              'criterion': criterions\n",
    "              \n",
    "        }\n",
    "grid_r = GridSearchCV(estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
    "                                              max_features='auto',\n",
    "                                              max_leaf_nodes=None,\n",
    "                                              min_impurity_decrease=0.0,\n",
    "                                              min_impurity_split=None,\n",
    "                                              min_samples_leaf=1,\n",
    "                                              min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,\n",
    "                                              n_jobs=None,\n",
    "                                              oob_score=False,\n",
    "                                              random_state=None, verbose=0,\n",
    "                                              warm_start=False),\n",
    "                                 param_grid=parameters,\n",
    "                                 cv=cv,\n",
    "                                 n_jobs = -1,scoring = 'roc_auc')\n",
    "grid_r.fit(train1,target)\n",
    "grid_rf = grid_r.best_estimator_\n",
    "grid_r.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "right-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993834324677208"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bagging \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "n_estimators = [10,30,50,70,80,150,160, 170,175,180,185]\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.30, random_state=15)\n",
    "\n",
    "parameters = {'n_estimators':n_estimators}\n",
    "grid_ba = GridSearchCV(BaggingClassifier(base_estimator=None, bootstrap=True,\n",
    "                                         bootstrap_features=False,\n",
    "                                         max_features=1.0, max_samples=1.0,\n",
    "                                         n_jobs=None,\n",
    "                                         oob_score=False, random_state=None,\n",
    "                                         verbose=0, warm_start=False),\n",
    "                                 param_grid=parameters,\n",
    "                                 cv=cv,\n",
    "                                 n_jobs = -1,scoring = 'roc_auc')\n",
    "grid_ba.fit(train1,target)\n",
    "grid_bag = grid_ba.best_estimator_\n",
    "grid_ba.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "pursuant-squad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6934462498186565"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adaboost ensemble model \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "n_estimators = [100,140,145,150,160, 170,175,180,185]\n",
    "learning_r = [0.1,1,0.01,0.5]\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.30, random_state=15)\n",
    "\n",
    "\n",
    "parameters = {'n_estimators':n_estimators,\n",
    "              'learning_rate':learning_r\n",
    "              \n",
    "        }\n",
    "grid_ad = GridSearchCV(AdaBoostClassifier(algorithm='SAMME.R',\n",
    "                                          base_estimator=None,\n",
    "                                          random_state=None),\n",
    "                                param_grid=parameters,\n",
    "                                 cv=cv,\n",
    "                                 n_jobs = -1,scoring = 'roc_auc')\n",
    "grid_ad.fit(train1,target)\n",
    "grid_ada = grid_ad.best_estimator_\n",
    "grid_ad.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cheap-devon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70668794429131"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient boost model \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.30, random_state=15)\n",
    "grad_accuracies = cross_val_score(gradient_boost, train1,target, cv = cv, scoring='roc_auc')\n",
    "grad_accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "color-murder",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter min_samples_split=1200 for estimator GradientBoostingClassifier(learning_rate=0.02, max_depth=4, max_features=17,\n                           min_samples_leaf=60, min_samples_split=800,\n                           n_estimators=70, random_state=10, subsample=0.9). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 581, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/Users/senlinlidewo/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\", line 230, in set_params\n    raise ValueError('Invalid parameter %s for estimator %s. '\nValueError: Invalid parameter min_samples_split=1200 for estimator GradientBoostingClassifier(learning_rate=0.02, max_depth=4, max_features=17,\n                           min_samples_leaf=60, min_samples_split=800,\n                           n_estimators=70, random_state=10, subsample=0.9). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-318f3adfa7ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.02, min_samples_split=800,n_estimators = 70,min_samples_leaf=60,max_depth=4,max_features=17,subsample=0.9,random_state=10), \n\u001b[1;32m      3\u001b[0m param_grid = param_test2, scoring='roc_auc',n_jobs=4, cv=5)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter min_samples_split=1200 for estimator GradientBoostingClassifier(learning_rate=0.02, max_depth=4, max_features=17,\n                           min_samples_leaf=60, min_samples_split=800,\n                           n_estimators=70, random_state=10, subsample=0.9). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "param_test2 = {'min_samples_split=1200':range(800,2000,200)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.09, min_samples_split=800,n_estimators = 70,min_samples_leaf=60,max_depth=4,max_features=17,subsample=0.9,random_state=10), \n",
    "param_grid = param_test2, scoring='roc_auc',n_jobs=4, cv=5)\n",
    "gsearch1.fit(train1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "surprised-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.09, min_samples_split=800,n_estimators = 70,min_samples_leaf=60,max_depth=4,max_features=17,subsample=0.9,random_state=10)\n",
    "gbc_model = gbc.fit(train1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "acting-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.drop(['isDefault','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cellular-banks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 102)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = test1.fillna(method='ffill')\n",
    "test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "champion-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = gbc_model.predict_proba(test1)\n",
    "probs = probs[:,1]\n",
    "ids = test.id.values\n",
    "prediction = pd.DataFrame({'id':ids,'isDefault':probs})\n",
    "prediction.head(5)\n",
    "prediction.to_csv('/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/submission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "flying-thing",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-973bee11068a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#basic nn model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "secure-visibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "f = open('/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/gbc_model.pickle','wb')\n",
    "pickle.dump(gbc_model,f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "healthy-honor",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-f47437ad2853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/gbc_model.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgbc_model1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbc_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "f = open('/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/gbc_model.pickle','rb')\n",
    "gbc_model1 = pickle.load(f)\n",
    "f.close()\n",
    "print(gbc_model1.predict(train1[0:1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "personal-marriage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.9}\n",
      "0.7268963954709584\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "sharp-rating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.09, max_depth=4, max_features='sqrt',\n",
       "                           min_samples_leaf=60, min_samples_split=800,\n",
       "                           n_estimators=70, random_state=10, subsample=0.8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = gsearch1.best_estimator_\n",
    "model.fit(train1,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "standing-picking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       499065\n",
       "1       338178\n",
       "2        97768\n",
       "3        80881\n",
       "5       281517\n",
       "         ...  \n",
       "9995    727332\n",
       "9996    717380\n",
       "9997    578410\n",
       "9998    726522\n",
       "9999    365564\n",
       "Name: id, Length: 9465, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "prostate-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:08:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "from xgboost import XGBClassifier\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.30, random_state=15)\n",
    "XGBClassifier = XGBClassifier()\n",
    "xgb = XGBClassifier.fit(train1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "anonymous-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7120883505005077\n"
     ]
    }
   ],
   "source": [
    "#extra tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ExtraTreesClassifier = ExtraTreesClassifier()\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n",
    "extra_accuracies = cross_val_score(ExtraTreesClassifier, train1,target, cv = cv, scoring='roc_auc')\n",
    "extra = ExtraTreesClassifier.fit(train1, target)\n",
    "print(extra_accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "checked-double",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gaussian process \n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=.30, random_state=15)\n",
    "gaussianp_accuracies = cross_val_score(GaussianProcessClassifier(), train1,target, cv = cv, scoring='roc_auc')\n",
    "GaussianProcessClassifier = GaussianProcessClassifier()\n",
    "gaussianprocess = GaussianProcessClassifier.fit(train1, target)\n",
    "gaussianp_accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "entitled-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:43:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:43:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:43:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# voting classifer \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('lr_grid', log_grid),\n",
    "    ('svc', grid_svc),\n",
    "    ('random_forest', grid_rf),\n",
    "    ('gradient_boosting', gradident),\n",
    "    ('decision_tree_grid',grid_dt),\n",
    "    ('knn_classifier', knn_grid),\n",
    "    ('XGB_Classifier', xgb),\n",
    "    ('bagging_classifier', grid_bag),\n",
    "    ('adaBoost_classifier',grid_ada),\n",
    "    ('ExtraTrees_Classifier', extra),\n",
    "    ('gaussian_classifier',gau),\n",
    "    ('gaussian_process_classifier', gaussianprocess)\n",
    "],voting='hard')\n",
    "\n",
    "#voting_classifier = voting_classifier.fit(train_x,train_y)\n",
    "voting_classifier = voting_classifier.fit(train1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [log_grid,grid_svc,grid_rf,grid_dt,knn_grid,xgb,grid_bag,grid_ada,extra,gau,gaussianprocess,voting_classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "functional-touch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{LogisticRegression(C=0.15): 0.5696746067179662,\n",
       " SVC(C=0.001, gamma=0.0001, probability=True): 0.5,\n",
       " RandomForestClassifier(criterion='entropy', max_depth=2, n_estimators=140): 0.7872928539538303,\n",
       " DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
       "                        max_depth=1, max_features=21, max_leaf_nodes=3,\n",
       "                        min_impurity_decrease=1.0, min_samples_leaf=4,\n",
       "                        random_state=666): 0.5,\n",
       " KNeighborsClassifier(n_jobs=-1, n_neighbors=1): 1.0,\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "               importance_type='gain', interaction_constraints='',\n",
       "               learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "               n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "               tree_method='exact', validate_parameters=1, verbosity=None): 0.7007761497171042,\n",
       " BaggingClassifier(n_estimators=30): 0.6993834324677208,\n",
       " AdaBoostClassifier(learning_rate=0.1, n_estimators=100): 0.6934462498186565,\n",
       " ExtraTreesClassifier(): 0.7200855940809516,\n",
       " GaussianNB(): 0.5368595214959162,\n",
       " GaussianProcessClassifier(): 0.5,\n",
       " VotingClassifier(estimators=[('lr_grid', LogisticRegression(C=0.15)),\n",
       "                              ('svc',\n",
       "                               SVC(C=0.001, gamma=0.0001, probability=True)),\n",
       "                              ('random_forest',\n",
       "                               RandomForestClassifier(criterion='entropy',\n",
       "                                                      max_depth=2,\n",
       "                                                      n_estimators=140)),\n",
       "                              ('gradient_boosting',\n",
       "                               GradientBoostingClassifier()),\n",
       "                              ('decision_tree_grid',\n",
       "                               DecisionTreeClassifier(class_weight='balanced',\n",
       "                                                      criterion='entropy',...\n",
       "                                             scale_pos_weight=1, subsample=1,\n",
       "                                             tree_method='exact',\n",
       "                                             validate_parameters=1,\n",
       "                                             verbosity=None)),\n",
       "                              ('bagging_classifier',\n",
       "                               BaggingClassifier(n_estimators=30)),\n",
       "                              ('adaBoost_classifier',\n",
       "                               AdaBoostClassifier(learning_rate=0.1,\n",
       "                                                  n_estimators=100)),\n",
       "                              ('ExtraTrees_Classifier', ExtraTreesClassifier()),\n",
       "                              ('gaussian_classifier', GaussianNB()),\n",
       "                              ('gaussian_process_classifier',\n",
       "                               GaussianProcessClassifier())]): 0.8391089108910892}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_models = [log_grid,grid_svc,grid_rf,grid_dt,knn_grid,xgb,grid_bag,grid_ada,extra,gau,gaussianprocess,voting_classifier]\n",
    "c = {}\n",
    "for i in all_models:\n",
    "    if i == log_grid:\n",
    "        a = i.decision_function(train1)\n",
    "        FPR, TPR, _ = roc_curve(target,a)\n",
    "        b = auc(FPR, TPR)\n",
    "        c[i] = b\n",
    "    elif i==grid_rf:\n",
    "        a = i.predict_proba(train1)[:,1]\n",
    "        FPR, TPR, _ = roc_curve(target,a)\n",
    "        b = auc(FPR, TPR)\n",
    "        c[i] = b\n",
    "    elif i == grid_bag:\n",
    "        c[i] = grid_ba.best_score_\n",
    "    elif i == grid_ada:\n",
    "        c[i] = grid_ad.best_score_\n",
    "    elif i == gradident:\n",
    "        c[i] = grad_accuracies.mean()\n",
    "    elif i == xgb:\n",
    "        c[i] = xgb_accuracies.mean()\n",
    "    elif i == extra:\n",
    "        c[i] = extra_accuracies.mean()\n",
    "    elif i ==gaussianprocess:\n",
    "        c[i] = gaussianp_accuracies.mean()\n",
    "    else:\n",
    "        a = i.predict(train1)\n",
    "        FPR, TPR, _ = roc_curve(target,a)\n",
    "        b = auc(FPR, TPR)\n",
    "        c[i] = b\n",
    "#a = i.predict(train1)\n",
    "#b = accuracy_score(a, target)\n",
    "    \n",
    "display(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-arcade",
   "metadata": {},
   "source": [
    "#这个输出格式暂时不对。\n",
    "test_prediction = (max(c, key=c.get)).predict(test1)\n",
    "submission = pd.DataFrame({\n",
    "        \"id\": id,\n",
    "        \"isdefault\": test_prediction\n",
    "    })\n",
    "\n",
    "#submission.id = submission.id.astype(int)\n",
    "#submission.isdefault = submission.isDefault.astype(int)\n",
    "\n",
    "submission.to_csv(\"/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/submission.csv\", index=False)\n",
    "\n",
    "result = pd.read_csv('/Users/senlinlidewo/pyprogram/天池竞赛-零基础金融风控/数据/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-mining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-bridal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
